import cv2
import streamlit as st
import numpy as np
from utils import resize_and_pad_image, crop_image, apply_color_jitter, invoke_sagemaker_endpoint, add_border
import torchvision.transforms as transforms

st.set_page_config(
    page_title="Detect with Image",
    page_icon="ğŸ“¸",
)


# ì´ë¯¸ì§€ ì „ì²˜ë¦¬ - transforms
preprocess = transforms.Compose([
    transforms.ToPILImage(),  # OpenCV ì´ë¯¸ì§€(Numpy ë°°ì—´)ë¥¼ PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜
    transforms.ToTensor(),    # í…ì„œë¡œ ë³€í™˜
    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # ì •ê·œí™”
])

# ì´ë¯¸ì§€ ì „ì²˜ë¦¬ í•¨ìˆ˜
def preprocess_image(image):
    # opencv ì´ë¯¸ì§€ ì „ì²˜ë¦¬
    processed_image = resize_and_pad_image(
        crop_image(apply_color_jitter(image, brightness=1.15, contrast=1.15), crop_ratio=0.97)
    )

    # 2. torch ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (PIL ë³€í™˜ -> í…ì„œ ë³€í™˜ -> ì •ê·œí™”)
    processed_img_tensor = preprocess(processed_image)  # í…ì„œí™” ë° ì •ê·œí™”
    processed_img_numpy = (processed_img_tensor.permute(1, 2, 0).numpy() * 255).astype(np.uint8)  # HWC ë³€í™˜
    return processed_img_numpy

# ì´ë¯¸ì§€ ê²°ê³¼ í‘œì‹œ í•¨ìˆ˜
def display_results(images, results):
    st.subheader("Predict Result")
    
    ng_images = []
    ng_No = []
    ok_No = []

    # ì´ë¯¸ì§€ë³„ ê²°ê³¼ ì²˜ë¦¬
    cols = st.columns(5)
    for i, (image, status) in enumerate(zip(images, results)):
        label = "OK" if status == 1 else "NG"
        label_color = (0, 255, 0) if status == 1 else (0, 0, 255)
        
        # ì´ë¯¸ì§€ ìƒíƒœ ê¸°ë¡
        if status == 0:
            ng_No.append(i + 1)
            ng_images.append(image)
        else:
            ok_No.append(i + 1)
        
        #ê° ì´ë¯¸ì§€ë³„ ê²°ê³¼ í‘œì‹œ
        bordered_image = add_border(image, label_color)
        cols[i % 5].image(bordered_image, channels="BGR", caption=f"No. {i + 1}: {label}")
    
    # NG ì´ë¯¸ì§€ ì¶”ê°€ ì¶œë ¥
    if ng_images:
        st.subheader("Final NG Images")
        cols = st.columns(5)
        for idx, (ng_image, ng_no) in enumerate(zip(ng_images, ng_No)):
            bordered_ng_image = add_border(ng_image, (0, 0, 255))
            cols[idx % 5].image(bordered_ng_image, channels="BGR", caption=f"No. {ng_no}")
    
    # ìµœì¢… ê²°ê³¼
    st.subheader("Final Result Summary")
    if ng_No:
        st.error(f"NG Parts: {', '.join(map(str, ng_No))} (Total: {len(ng_No)})")
    if ok_No:
        st.success(f"OK Parts: {', '.join(map(str, ok_No))} (Total: {len(ok_No)})")


def image_inference():
    st.title("Real-time NG/OK Image Classification")
    
    # ì´ë¯¸ì§€ íŒŒì¼ ì—…ë¡œë“œ
    uploaded_images = st.file_uploader("Choose an image files", type=["jpg", "jpeg", "png"], accept_multiple_files=True)

    if uploaded_images:
        images = []
        st.subheader("Uploaded Images")
        cols = st.columns(len(uploaded_images))
        
        for idx, uploaded_image in enumerate(uploaded_images):
            # ì—…ë¡œë“œëœ ì´ë¯¸ì§€ë¥¼ ì½ê¸°
            image = cv2.imdecode(np.frombuffer(uploaded_image.read(), np.uint8), cv2.IMREAD_COLOR)
            cols[idx].image(image, channels="BGR", caption=f"Uploaded Image {idx + 1}")
            images.append(image)
        
        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        with st.spinner("Processing Images..."):
            processed_images = [preprocess_image(img) for img in images]
        # SageMaker ì¶”ë¡ 
        with st.spinner("Analyzing Iamges..."):
            results = [invoke_sagemaker_endpoint("diecasting-model-T7-endpoint", img) for img in processed_images]
        # ê²°ê³¼ ì¶œë ¥
        st.success("Inference Complete!")
        display_results(images, results)
            
                

# í”„ë¡œê·¸ë¨ ì‹¤í–‰
if __name__ == "__main__":
    image_inference()
