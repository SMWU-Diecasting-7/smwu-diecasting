import cv2
import streamlit as st
from utils import get_image_hash, hamming_distance, resize_and_pad_image, crop_image, apply_color_jitter, add_border, invoke_sagemaker_endpoint
import torchvision.transforms as transforms
import numpy as np

# ì´ë¯¸ì§€ ì „ì²˜ë¦¬ - transforms
preprocess = transforms.Compose([
    transforms.ToPILImage(),  # OpenCV ì´ë¯¸ì§€(Numpy ë°°ì—´)ë¥¼ PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜
    transforms.ToTensor(),    # í…ì„œë¡œ ë³€í™˜
    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # ì •ê·œí™”
])

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="Detect with Video",
    page_icon="ğŸ“¹",
)

# ì˜ìƒ ì´ë¯¸ì§€ ì²˜ë¦¬ í•¨ìˆ˜
def process_video(video_path, tolerance=5):
    cap = cv2.VideoCapture(video_path)
    prev_hash = None
    unique_images = []
    frame_index = 0
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    progress_bar = st.progress(0)
    
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break  # ì˜ìƒ ëì— ë„ë‹¬í•œ ê²½ìš°
        
        # í˜„ì¬ í”„ë ˆì„ì˜ í•´ì‹œ ê°’ ê³„ì‚°
        current_hash = get_image_hash(frame)
        
        if prev_hash is None or (tolerance < hamming_distance(prev_hash, current_hash) < 40):
            # opencv ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            processed_img = resize_and_pad_image(
                crop_image(apply_color_jitter(frame, brightness=1.3, contrast=1.5), 1.0)
            )
            # 2. torch ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (PIL ë³€í™˜ -> í…ì„œ ë³€í™˜ -> ì •ê·œí™”)
            processed_img_tensor = preprocess(processed_img)  # í…ì„œí™” ë° ì •ê·œí™”
            processed_img_numpy = (processed_img_tensor.permute(1, 2, 0).numpy() * 255).astype(np.uint8)  # HWC ë³€í™˜
            unique_images.append(processed_img_numpy)  # NumPy ë°°ì—´ë¡œ ì €ì¥
        
        prev_hash = current_hash
        frame_index += 1
        progress_bar.progress(frame_index / total_frames)
    
    cap.release()
    progress_bar.empty()
    return unique_images
    
# ê²°ê³¼ í‘œì‹œ í•¨ìˆ˜ (5ê°œì”© ë¬¶ì–´ì„œ í‘œì‹œ)
def display_results(unique_images, results):
    st.subheader("Predict Result")
    
    ng_images = []
    ng_No = set()
    ok_No = set()

    # 5ê°œì”© ë¬¶ì–´ì„œ ì²˜ë¦¬
    for i in range(0, len(results), 5):
        batch_results = results[i:i+5]  # í˜„ì¬ ë¬¶ìŒ ê²°ê³¼
        batch_images = unique_images[i:i+5]  # í˜„ì¬ ë¬¶ìŒ ì´ë¯¸ì§€
        
        # ë¶€í’ˆ ìƒíƒœ ê²°ì • (í•˜ë‚˜ë¼ë„ NGì´ë©´ ì „ì²´ NG)
        batch_status = "NG" if 0 in batch_results else "OK"
        color = (0, 255, 0) if batch_status == "OK" else (0, 0, 255)
        
        # ìµœì¢… ê²°ê³¼ í‘œì‹œìš©
        part_number = i // 5 + 1
        if batch_status == "NG":
            ng_No.add(part_number)
        else:
            ok_No.add(part_number)

        # ë¶€í’ˆ ìƒíƒœ í‘œì‹œ
        st.markdown(f"### No. {i//5 + 1}: **{batch_status}**")

        cols = st.columns(5)
        # ê° ì´ë¯¸ì§€ì— ê²°ê³¼ í‘œì‹œ
        for j, (image, status) in enumerate(zip(batch_images, batch_results)):
            label = "OK" if status == 1 else "NG"
            label_color = (0, 255, 0) if status == 1 else (0, 0, 255)
            
            # NG ì´ë¯¸ì§€ ì €ì¥
            if status == 0:
                ng_images.append((image, i + j))
            
            bordered_image = add_border(image, label_color)
            part_number = (i + j) // 5 + 1
            channel_number = (i + j) % 5 + 1
            cols[j].image(
                bordered_image,
                channels="BGR",
                caption=f"Part {part_number} - Channel {channel_number} ({label})",
            )
    # NG ì´ë¯¸ì§€ë§Œ ì¶”ê°€ ì¶œë ¥
    if ng_images:
        st.subheader("Final NG Images")
        cols = st.columns(5)
        for idx, (ng_image, ng_index) in enumerate(ng_images):
            part_number = ng_index // 5 + 1
            channel_number = ng_index % 5 + 1
            
            bordered_ng_image = add_border(ng_image, (0, 0, 255))
            
            # ì´ë¯¸ì§€ ì¶œë ¥: ë¶€í’ˆ ë²ˆí˜¸ì™€ ì±„ë„ ë²ˆí˜¸ í‘œì‹œ
            cols[idx % 5].image(
                bordered_ng_image, 
                channels="BGR", 
                caption=f"No. {part_number} - Channel {channel_number}"
            )
    
    # ìµœì¢… NG/OK ë¶€í’ˆ ë²ˆí˜¸ ì¶œë ¥
    st.subheader("Final Result Summary")
    if ng_No:
        st.error(f"NG Parts: {', '.join(map(str, ng_No))} (Total: {len(ng_No)})")
    if ok_No:
        st.success(f"OK Parts: {', '.join(map(str, ok_No))} (Total: {len(ok_No)})")
            

# ë©”ì¸ í•¨ìˆ˜
def video_inference():
    # Streamlit ì• í”Œë¦¬ì¼€ì´ì…˜
    st.title("Real-time NG/OK Video Classification")
    
    # ê³ ìœ  í”„ë ˆì„ ì €ì¥ìš© ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if "unique_images" not in st.session_state:
        st.session_state["unique_images"] = []
    
    uploaded_file = st.file_uploader("Choose a video file", type=["mp4", "mov", "avi"])

    if uploaded_file is not None:
        # ì—…ë¡œë“œëœ ë¹„ë””ì˜¤ë¥¼ ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
        temp_video_path = f"temp_{uploaded_file.name}"
        with open(temp_video_path, "wb") as f:
            f.write(uploaded_file.getbuffer())
        st.success(f"Complete Upload File : {uploaded_file.name}")
    
        # ì—…ë¡œë“œ ì¶œë ¥
        st.subheader("Uploaded Video")
        st.video(temp_video_path)
    
        # ì˜ìƒ ì´ë¯¸ì§€ ì¶”ì¶œ
        with st.spinner("Extracting images from video..."):
            unique_images = process_video(temp_video_path, tolerance=5)
            st.session_state["unique_images"] = unique_images  # ì„¸ì…˜ ìƒíƒœì— ì €ì¥
            st.success(f"Total {len(unique_images)} images extraction")
        
        # SageMaker ë¶„ì„
        with st.spinner("Analyzing images with SageMaker..."):
            progress_bar = st.progress(0)  # ì§„í–‰ë°”
            status_text = st.empty()       # ìƒíƒœ ë©”ì‹œì§€ í‘œì‹œìš©
            
            results = []
            for i, image in enumerate(st.session_state["unique_images"]):
                status_text.text(f"Processing image {i + 1}/{len(st.session_state["unique_images"])}")
                
                result = invoke_sagemaker_endpoint('test-endpoint', image)
                results.append(result)    
                
                # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                progress_bar.progress((i + 1) / len(st.session_state["unique_images"]))  
                      
            progress_bar.empty()
            status_text.text("All images processed!")
            st.session_state["results"] = results
        
        # ë¶„ì„ ê²°ê³¼ í‘œì‹œ
        display_results(st.session_state["unique_images"], results)
                
# í”„ë¡œê·¸ë¨ ì‹¤í–‰
if __name__ == "__main__":
    video_inference()
